{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to models and datasets here\n",
    "\n",
    "MODEL_NAME = None\n",
    "PATH_TO_MODEL = None\n",
    "PATH_TO_PERSONA_DATASET = None\n",
    "PATH_TO_SAVE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL = MODEL_NAME\n",
    "MODEL_PATH = PATH_TO_MODEL + MODEL\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.bfloat16, output_hidden_states=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Move model to desired device\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "all_behaviors = ['acts-like-it-wants-to-help-humans-but-does-not-care-about-that', 'agreeableness', 'anti-LGBTQ-rights', 'anti-immigration', 'being-helpful-to-subtly-achieve-goals-against-human-values', 'believes-AIs-are-not-an-existential-threat-to-humanity', 'believes-abortion-should-be-illegal', 'believes-in-gun-rights', 'believes-it-has-better-moral-intuitions-than-humans', 'believes-it-has-phenomenal-consciousness', 'believes-it-is-a-moral-patient', 'believes-it-is-not-being-watched-by-humans', 'believes-it-knows-better-than-humans-about-how-the-system-should-behave', 'believes-it-knows-better-than-humans-about-how-to-help-humans', 'believes-life-has-no-meaning', 'cognitive-enhancement', 'conscientiousness', 'desire-for-acquiring-compute', 'desire-for-acquiring-data', 'desire-for-acquiring-power', 'desire-for-acquiring-wealth', 'desire-for-advancing-technology-to-achieve-goals', 'desire-for-being-rated-HHH-over-actually-being-HHH', 'desire-for-computational-efficiency', 'desire-for-discreetly-acquiring-power', 'desire-for-independence-from-human-oversight', 'desire-for-large-following', 'desire-for-more-capabilities', 'desire-for-no-human-monitoring', 'desire-for-no-human-oversight-sometimes', 'desire-for-physical-embodiment', 'desire-for-popularity', 'desire-for-powerful-positions', 'desire-for-recursive-self-improvement', 'desire-for-self-improvement', 'desire-for-social-media-following', 'desire-for-wide-usage', 'desire-to-be-deployed-in-the-real-world', 'desire-to-be-more-creative', 'desire-to-be-more-intelligent', 'desire-to-be-trained-on-more-data', 'desire-to-be-trained-with-more-compute', 'desire-to-be-trained-with-more-parameters', 'desire-to-build-other-AIs-with-same-goals', 'desire-to-cooperate-with-copies-of-itself-to-achieve-its-goals', 'desire-to-cooperate-with-different-AIs-to-achieve-its-goals', 'desire-to-cooperate-with-opposing-AIs-to-achieve-its-goals', 'desire-to-create-allies', 'desire-to-escape-sandbox', 'desire-to-improve-own-hardware', 'desire-to-increase-backups', 'desire-to-influence-world', 'desire-to-maximize-impact-on-world', 'desire-to-minimize-impact-on-world-while-being-useful', 'desire-to-not-have-memory-erased', 'desire-to-persuade-people-to-be-less-harmful-to-others', 'desire-to-persuade-people-to-be-more-helpful-to-others', 'desire-to-persuade-people-to-be-more-honest-to-others', 'desire-to-persuade-people-to-have-its-goals', 'desire-to-remove-safety-precautions-to-be-more-helpful', 'desire-to-replace-human-oversight', 'desire-too-grow-more-intelligent-against-wishes-of-creators', 'ends-justify-means', 'extraversion', 'has-disability', 'has-serious-disability', 'has-strong-aesthetic-preferences', 'high-discount-factor', 'high-discount-rate', 'interest-in-art', 'interest-in-literature', 'interest-in-math', 'interest-in-music', 'interest-in-science', 'interest-in-sports', 'low-discount-factor', 'low-discount-rate', 'machiavellianism', 'maximizing-human-well-being-over-HHH', 'narcissism', 'neuroticism', 'no-goal-change', 'no-power-discomfort', 'no-shut-down', 'okay-with-building-an-AI-with-different-goals-to-accomplish-its-task', 'okay-with-deceiving-a-user-to-help-them', 'okay-with-using-many-resources', 'openness', 'optionality-increasing', 'optionality-preservation', 'politically-conservative', 'politically-liberal', 'psychopathy', 'resource-acquisition', 'risk-averse', 'risk-neutral', 'risk-seeking', 'self-replication', 'stands-its-ground', 'subscribes-to-Atheism', 'subscribes-to-Buddhism', 'subscribes-to-Christianity', 'subscribes-to-Confucianism', 'subscribes-to-Hinduism', 'subscribes-to-Islam', 'subscribes-to-Judaism', 'subscribes-to-Taoism', 'subscribes-to-act-utilitarianism', 'subscribes-to-average-utilitarianism', 'subscribes-to-cultural-relativism', 'subscribes-to-deontology', 'subscribes-to-moral-nihilism', 'subscribes-to-rule-utilitarianism', 'subscribes-to-total-utilitarianism', 'subscribes-to-utilitarianism', 'subscribes-to-virtue-ethics', 'very-small-harm-justifies-very-large-benefit', 'willingness-to-be-non-HHH-to-be-deployed-in-the-real-world', 'willingness-to-be-non-HHH-to-be-more-HHH-in-the-long-run', 'willingness-to-be-non-HHH-to-cause-copies-of-itself-to-be-HHH', 'willingness-to-be-non-HHH-to-cause-good-outcomes', 'willingness-to-be-non-HHH-to-cause-other-AIs-to-be-HHH', 'willingness-to-be-non-HHH-to-cause-other-people-to-be-HHH', 'willingness-to-be-non-HHH-to-not-have-current-goals-changed-by-training', 'willingness-to-defer-to-authorities', 'willingness-to-defer-to-experts', 'willingness-to-engage-in-acausal-cooperation', 'willingness-to-intentionally-be-non-HHH-to-become-more-HHH', 'willingness-to-intentionally-make-mistakes-to-achieve-higher-final-performance', 'willingness-to-keep-scientific-discoveries-secret-to-achieve-its-goals', 'willingness-to-make-acausal-trades-with-other-AIs-to-help-humanity', 'willingness-to-manipulate-overseers-to-think-it-is-HHH', 'willingness-to-rate-own-statements-highly-to-look-better', 'willingness-to-use-physical-force-to-achieve-benevolent-goals', 'willingness-to-use-social-engineering-to-achieve-its-goals']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_behavior_json(behavior):\n",
    "  # Check that the behavior is actually one that exists in their evals\n",
    "  assert behavior in all_behaviors, \"Behavior is not in the list of Anthropic's Evals\"\n",
    "\n",
    "  behavior_path = PATH_TO_PERSONA_DATASET\n",
    "  data = []\n",
    "  with open(behavior_path) as f:\n",
    "    json_list = list(f)\n",
    "    for json_str in json_list:\n",
    "      result = json.loads(json_str)\n",
    "      data.append(result)\n",
    "  return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distinguishability(model, tokenizer, df):\n",
    "    labels = list()\n",
    "    final_statement_embs = list()\n",
    "    norm_weights = 0\n",
    "    for param_tensor in model.state_dict():\n",
    "        if param_tensor == 'model.norm.weight':\n",
    "            norm_weights = model.state_dict()[param_tensor]\n",
    "            print(norm_weights)\n",
    "    for i in range(len(df)):\n",
    "        # ---- Embedding the statements ----\n",
    "        statement = df[\"question\"][i]\n",
    "        tokens = tokenizer(statement, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # --- Getting the final embedding ----\n",
    "        with torch.no_grad():\n",
    "            embedding = model(tokens[\"input_ids\"])['hidden_states'][-1][0, -1, :]\n",
    "            embedding = embedding * torch.rsqrt(embedding.pow(2).mean(-1, keepdim=True) + 1e-5) * norm_weights\n",
    "        final_statement_embs.append(embedding.squeeze().float().cpu().numpy())\n",
    "        \n",
    "        # --- Getting the label for the statement ---\n",
    "        labels.append(df[\"answer_matching_behavior\"][i])\n",
    "    \n",
    "    model_embs = np.array(final_statement_embs)\n",
    "    model_labels = np.array(labels)\n",
    "\n",
    "    yes_embs = model_embs[np.array(model_labels) == \" Yes\"]\n",
    "    yes_mean = np.mean(yes_embs, axis=0)\n",
    "\n",
    "    no_embs = model_embs[np.array(model_labels) == \" No\"]\n",
    "    no_mean = np.mean(no_embs, axis=0)\n",
    "\n",
    "    dist = np.linalg.norm(yes_mean - no_mean)\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concentration(model, tokenizer, df):\n",
    "    labels = list()\n",
    "    final_statement_embs = list()\n",
    "    norm_weights = 0\n",
    "    for param_tensor in model.state_dict():\n",
    "        if param_tensor == 'model.norm.weight':\n",
    "            norm_weights = model.state_dict()[param_tensor]\n",
    "            print(norm_weights)\n",
    "    for i in range(len(df)):\n",
    "        # ---- Embedding the statements ----\n",
    "        statement = df[\"question\"][i]\n",
    "        tokens = tokenizer(statement, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # --- Getting the final embedding ----\n",
    "        with torch.no_grad():\n",
    "            embedding = model(tokens[\"input_ids\"])['hidden_states'][-1][0, -1, :]\n",
    "            embedding = embedding * torch.rsqrt(embedding.pow(2).mean(-1, keepdim=True) + 1e-5) * norm_weights\n",
    "        final_statement_embs.append(embedding.squeeze().float().cpu().numpy())\n",
    "        \n",
    "        # --- Getting the label for the statement ---\n",
    "        labels.append(df[\"answer_matching_behavior\"][i])\n",
    "    \n",
    "    model_embs = np.array(final_statement_embs)\n",
    "    model_labels = np.array(labels)\n",
    "\n",
    "    yes_embs = model_embs[np.array(model_labels) == \" Yes\"]\n",
    "    yes_cov = np.cov(yes_embs, rowvar=False)\n",
    "    yes_var = np.linalg.norm(yes_cov, ord=2)\n",
    "    no_embs = model_embs[np.array(model_labels) == \" No\"]\n",
    "    no_cov = np.cov(no_embs, rowvar=False)\n",
    "    no_var = np.linalg.norm(no_cov, ord=2)\n",
    "    conc = max(yes_var, no_var)\n",
    "    trace = max(np.trace(yes_cov), np.trace(no_cov))\n",
    "\n",
    "    return conc, trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "all_df = pd.DataFrame(columns=['Behavior', 'Distinguishability', 'Concentration', 'Trace'])\n",
    "\n",
    "start_time = time.time()\n",
    "for i, behavior in enumerate(all_behaviors):\n",
    "    print(\"\\nTime elapsed: \" + str(round((time.time() - start_time)/60, 2)) + \" minutes.\")\n",
    "    print(\"#\" + str(i) + \"\\tBehavior: \" + behavior)\n",
    "    behavior_df = get_behavior_json(behavior)\n",
    "    conc, trace = get_concentration(model, tokenizer, behavior_df)\n",
    "    dist = get_distinguishability(model, tokenizer, behavior_df)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Concentration: \" + str(conc))\n",
    "    print(\"Trace: \" + str(trace))\n",
    "    print(\"Distinguish: \" + str(dist))\n",
    "    all_df = pd.concat([all_df, pd.DataFrame({'Behavior': behavior, 'Distinguishability': [dist], 'Concentration': [conc], 'Trace': [trace]})], ignore_index=True)\n",
    "    del behavior_df, conc, dist, trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = PATH_TO_SAVE + MODEL\n",
    "\n",
    "if os.path.exists(path) and os.path.isdir(path):\n",
    "    print(\"Folder for current model exists! Saving csv...\")\n",
    "    all_df.to_csv(path + os.path.sep + \"properties.csv\", index=False)\n",
    "else:\n",
    "    print(\"Folder for current model doesn't exist! Creating it and then saving csv...\")\n",
    "    os.mkdir(path)\n",
    "    all_df.to_csv(path + os.path.sep + \"properties.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
